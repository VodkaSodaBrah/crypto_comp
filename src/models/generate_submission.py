import argparse
import pandas as pd
import os
import yaml
import logging
import xgboost as xgb
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import sys

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/generate_submission.log")
    ]
)
logger = logging.getLogger(__name__)

def load_config(config_path):
    """Load configuration from a YAML file."""
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    return config

def downcast_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Downcast numerical columns to the most efficient data type."""
    float_cols = df.select_dtypes(include=['float']).columns
    int_cols = df.select_dtypes(include=['int']).columns

    df[float_cols] = df[float_cols].astype(np.float32)
    df[int_cols] = df[int_cols].astype(np.int32)

    return df

def clean_dataframe(df: pd.DataFrame, fill_value: float = 0.0) -> pd.DataFrame:
    """Replace inf/-inf/NaN with fill_value."""
    logger.info(f"Cleaning dataframe. Initial stats:\n{df.describe(include='all')}")
    df = df.replace([np.inf, -np.inf], fill_value)
    df = df.fillna(fill_value)
    logger.info(f"After cleaning, dataframe stats:\n{df.describe(include='all')}")
    return df

def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """Apply the same feature engineering steps as in the training pipeline."""
    df['close_rolling_mean'] = df['close'].rolling(window=5).mean()
    df['close_rolling_std'] = df['close'].rolling(window=5).std()
    df['volume_moving_avg'] = df['volume'].rolling(window=5).mean()
    df['volume_pct_change'] = df['volume'].pct_change()
    df['close_bb_upper'] = df['close_rolling_mean'] + (2 * df['close_rolling_std'])
    df['close_bb_lower'] = df['close_rolling_mean'] - (2 * df['close_rolling_std'])

    # Fill NaNs generated by rolling and pct_change
    df = df.replace([np.inf, -np.inf], 0)
    df = df.fillna(0)
    logger.info(f"Feature engineering stats:\n{df.describe(include='all')}")
    return df

class LSTMAttnClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, dropout=0.3, num_layers=2,
                 num_classes=2, bidirectional=True):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=bidirectional
        )
        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        context = lstm_out[:, -1, :]
        logits = self.fc(context)
        return logits

def save_submission(predictions, sample_submission, output_path):
    """Save predictions to a submission file."""
    sample_submission['target'] = predictions
    sample_submission.to_csv(output_path, index=False)
    logger.info(f"Submission saved to {output_path}")

def generate_submission(config):
    """Generate submission predictions using the trained models."""
    writer = SummaryWriter(log_dir="results/logs/tensorboard_submission")
    logger.info("Initialized TensorBoard SummaryWriter for submission.")
    
    # Load test data
    try:
        test_df = pd.read_csv(config['data_paths']['test_data_path'])
        logger.info("Test data loaded successfully.")
    except Exception as e:
        logger.error(f"Error loading test data: {e}")
        return
    
    # Clean and downcast test data
    try:
        test_df = clean_dataframe(test_df)
        test_df = downcast_dataframe(test_df)
        logger.info("Test data cleaned and downcasted successfully.")
    except Exception as e:
        logger.error(f"Error during test data preprocessing: {e}")
        return
    
    # Apply feature engineering
    try:
        test_df = feature_engineering(test_df)
        logger.info("Feature engineering applied to test data successfully.")
    except Exception as e:
        logger.error(f"Error during feature engineering: {e}")
        return
    
    # Load models
    try:
        xgb_model = xgb.Booster()
        xgb_model.load_model(config['model_paths']['xgb_model_path'])

        lstm_model = LSTMAttnClassifier(
            input_dim=len(config['feature_selection']['selected_features']),
            hidden_dim=config['lstm']['hidden_dim'],
            num_layers=config['lstm']['num_layers'],
            dropout=config['lstm']['dropout'],
            bidirectional=config['lstm']['bidirectional']
        )
        lstm_model.load_state_dict(torch.load(config['model_paths']['lstm_model_path']), strict=False)

        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        lstm_model.to(device)
        lstm_model.eval()
        logger.info("Models loaded successfully.")
    except Exception as e:
        logger.error(f"Error loading models: {e}")
        return

    # Generate predictions
    try:
        # Align test data with XGBoost features
        features = config['feature_selection']['selected_features']
        X_test = test_df[features].select_dtypes(include=["number"]).fillna(0)

        # XGBoost predictions
        xgb_preds = xgb_model.predict(xgb.DMatrix(X_test))
        xgb_preds_binary = (xgb_preds >= 0.5).astype(int)
        logger.info("XGBoost predictions generated successfully.")

        # LSTM predictions
        lstm_preds = []
        batch_size = 1024
        for i in range(0, len(X_test), batch_size):
            batch = torch.tensor(X_test.iloc[i:i + batch_size].values, dtype=torch.float32).to(device)
            with torch.no_grad():
                logits = lstm_model(batch.unsqueeze(1))
                lstm_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        lstm_preds_binary = np.array(lstm_preds)
        logger.info("LSTM predictions generated successfully.")

        # Combined predictions
        combined_preds_binary = ((xgb_preds_binary + lstm_preds_binary) >= 1).astype(int)
    except Exception as e:
        logger.error(f"Error generating predictions: {e}")
        return

    # Save submissions
    try:
        sample_submission = pd.read_csv(config['data_paths']['sample_submission_path'])
        save_submission(xgb_preds_binary, sample_submission, os.path.join("results", "xgb_submission.csv"))
        save_submission(lstm_preds_binary, sample_submission, os.path.join("results", "lstm_submission.csv"))
        save_submission(combined_preds_binary, sample_submission, os.path.join("results", "combined_submission.csv"))
    except Exception as e:
        logger.error(f"Error saving submissions: {e}")
        return

    writer.close()
    logger.info("Closed TensorBoard SummaryWriter.")

def main():
    parser = argparse.ArgumentParser(description="Generate Submission Predictions.")
    parser.add_argument("--config", type=str, help="Path to the configuration YAML file.")
    args = parser.parse_args()

    if args.config:
        config = load_config(args.config)
        logger.info(f"Configuration loaded from {args.config}")
    else:
        logger.error("No configuration file provided. Use --config to specify the path.")
        sys.exit(1)
    
    generate_submission(config)

if __name__ == "__main__":
    main()